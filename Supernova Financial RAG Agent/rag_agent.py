# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cTBkRq9OUuazm86DPuo1UgkZ4x_DB7sk
"""

!pip install -q \
    faiss-cpu \
    langchain \
    langchain-community \
    sentence-transformers \
    transformers \
    accelerate \
    pypdf

from google.colab import files

uploaded = files.upload()  # Select your PDF file here
pdf_path = list(uploaded.keys())[0]

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = PyPDFLoader(pdf_path)
documents = loader.load()

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)
print(f"Total chunks created: {len(docs)}")

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.from_documents(docs, embedding_model)

retriever = db.as_retriever(search_kwargs={"k": 3})

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline

# âœ… Use your token here
HF_TOKEN = "########################"

model_id = "mistralai/Mistral-7B-Instruct-v0.1"

# Load tokenizer and model securely
tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",            # Use GPU if available
    torch_dtype="auto",           # Efficient memory handling
    token=HF_TOKEN
)

# Create pipeline
llm_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.7,
    top_p=0.9
)

# LangChain wrapper for use in RAG
llm = HuggingFacePipeline(pipeline=llm_pipeline)

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"  # injects all retrieved docs into prompt
)

query = "What steps are required before an expense is reimbursed according to Supernovaâ€™s policy?"
response = qa_chain.run(query)

# Clean the output to remove backend/excess prompt details
clean_response = response.strip()

# If your LLM response includes "Helpful Answer:", split and take the relevant part
if "Helpful Answer:" in clean_response:
    clean_response = clean_response.split("Helpful Answer:")[-1].strip()

# Optional: If "Answer:" prefix exists, handle that too
if "Answer:" in clean_response:
    clean_response = clean_response.split("Answer:")[-1].strip()

print("ðŸ’¬ Answer:\n", clean_response)

import gradio as gr
from langchain.chains import RetrievalQA

# Your qa_chain must be already created using LangChain
# Example:
# qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

def chat_fn(message, history):
    response = qa_chain.run(message)
    return response

gr.ChatInterface(
    fn=chat_fn,
    title="ðŸ’¼ Supernova Financial Chatbot",
    description="Made by Shubham Singh | Powered by Mistral-7B",
    theme="default",
    examples=["What is the financial summary in the PDF?", "Explain the risk management strategy?", "Tell me the key ratios"],
).launch(share=True)